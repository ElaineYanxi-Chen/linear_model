---
title: "Cross Validation"
author: "Elaine Yanxi Chen"
date: "`r Sys.Date()`"
output: github_document
---

## Packages and settings

First we load the packages necessary to knit this document.

```{r packages and settings, message = FALSE}
library(tidyverse)
library(mgcv)
library(modelr)
library(viridis)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```


## Step one

Cross validation "by hand" on simulated data.

We first generate a non-linear model by hand.

```{r}
nonlin_df = 
  tibble(
    id = 1:100,
    x = runif(100, 0, 1),
    y = 1 - 10 * (x - 0.3) ^ 2 + rnorm(100, 0, 0.3)
  )

nonlin_df %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point()
```


Let's get this by hand.Split the data into training and test sets using `anti_join` and replot showing the split. We want to use the training data in black to build candidate models, then see how these models predict in the testing data in red.

```{r}
train_df = sample_n(nonlin_df, 80)
test_df = anti_join(nonlin_df, train_df, by = "id")
```

```{r}
train_df %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_point(data = test_df, colour = "red")
```


Let's try to fit three models. 

Using `mgcv::gam` for non-linear models, drawing smooth lines through data clouds, and can control how smooth we want the fit to be.

```{r}
linear_mod = lm(y ~ x, data = train_df)
smooth_mod = mgcv::gam(y ~ s(x), data = train_df)
wiggly_mod = mgcv::gam(y ~ s(x, k = 30), sp = 10e-6, data = train_df)
```


Let's see the results.

```{r}
train_df %>% 
  add_predictions(linear_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), colour = "red")

train_df %>% 
  add_predictions(smooth_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), colour = "red")

train_df %>% 
  add_predictions(wiggly_mod) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), colour = "red")
```

Add predictions with `modelr::gather_predictions` function for several models to a dataframe and then pivot so that the result is tidy.

```{r}
train_df %>% 
  gather_predictions(linear_mod, smooth_mod, wiggly_mod) %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(y = pred), colour = "red") +
  facet_wrap(~ model)
```

Linear model is too simple and will never capture the true relationship between variables no matter what trianing data we use, standard smooth fit is pretty good, wiggly fit is too complex, chasing datapoints, vary a lot from one training dataset to the next, consistently wrong due to its complexity, highly variable

Let's make predictions and compute RMSEs.

```{r}
test_df %>% add_predictions(linear_mod)

rmse(linear_mod, test_df)
rmse(smooth_mod, test_df)
rmse(wiggly_mod, test_df)
```


## Can we iterate...?

We needed to convert output from `crossv_mc` to a tibble because it is not compatible with `gam`. If all we want is models from `lm` fit then this step can be skipped. 

Here, we are fitting each training dataset through the three models: linear, smooth, and wiggly.And then we will obtain the RMSEs by fitting each of the model through the testing dataset.

```{r}
cv_df = 
  crossv_mc(nonlin_df, 100)

cv_df %>% pull(train) %>% .[[1]] %>% as_tibble()

cv_df = 
  crossv_mc(nonlin_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble),
  ) %>% 
  mutate(
    linear_fits = map(.x = train, ~lm(y ~ x, data = .x)),
    smooth_fits = map(.x = train, ~mgcv::gam(y ~ s(x), data = .x)),
    wiggly_fits = map(.x = train, ~mgcv::gam(y ~ s(x, k = 30), sp = 10e-6, data = .x))
  ) %>% 
  mutate(
    rmse_linear = map2_dbl(linear_fits, test, ~rmse(model = .x, data = .y)),
    rmse_smooth = map2_dbl(smooth_fits, test, ~rmse(model = .x, data = .y)),
    rmse_wiggly = map2_dbl(wiggly_fits, test, ~rmse(model = .x, data = .y)),
  )
```


Make a box plot...
Compare these models using the RMSE. 

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot()
```


## Try it on a real dataset

```{r}
growth_df = read_csv("data/nepalese_children.csv")
```


```{r}
growth_df %>% 
  ggplot(aes(x = weight, y = armc)) +
  geom_point(alpha = 0.3)
```

The plot suggest some non-linearity, especially at the low end of the weight distribution. The three models we will try here are the linear fit, a piecewise linear fit, and a smooth fit using `gam`.

Brief aside on piecewise linear models.

Here we add a "change point term" to the dataframe.

```{r}
child_growth =
  growth_df %>% 
  mutate(
    weight_pwl = (weight > 7) * (weight - 7)
  )
```


```{r}
linear_model = lm(armc ~ weight, data = child_growth)
pwl_model = lm(armc ~ weight + weight_pwl, data = child_growth)
smooth_model = mgcv::gam(armc ~ s(weight), data = child_growth)
```

Plotting the three models for an intuition for goodness of fit.

```{r}
child_growth %>% 
  gather_predictions(linear_model, pwl_model, smooth_model) %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = weight, y = armc)) + 
  geom_point(alpha = .5) +
  geom_line(aes(y = pred), colour = "red") + 
  facet_grid(~model)
```

```{r}
child_growth %>% 
  add_predictions(pwl_model) %>% 
  ggplot(aes(x = weight, y = armc)) +
  geom_point(alpha = 0.3) +
  geom_line(aes(y = pred), colour = "red")

child_growth %>% 
  add_predictions(linear_model) %>% 
  ggplot(aes(x = weight, y = armc)) +
  geom_point(alpha = 0.3) +
  geom_line(aes(y = pred), colour = "red")

child_growth %>% 
  add_predictions(smooth_model) %>% 
  ggplot(aes(x = weight, y = armc)) +
  geom_point(alpha = 0.3) +
  geom_line(aes(y = pred), colour = "red")
```


```{r}
cv_df = 
  crossv_mc(child_growth, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble),
  )

cv_df = 
  cv_df %>% 
  mutate(
    linear_fits = map(.x = train, ~lm(armc ~ weight, data = .x)),
    pwl_fits =    map(.x = train, ~lm(armc ~ weight + weight_pwl, data = .x)),
    smooth_fits = map(.x = train, ~mgcv::gam(armc ~ s(weight), data = .x))
  ) %>% 
  mutate(
    rmse_linear = map2_dbl(.x = linear_fits, .y = test, ~rmse(model = .x, data = .y)),
    rmse_pwl =    map2_dbl(.x = pwl_fits, .y = test, ~rmse(model = .x, data = .y)),
    rmse_smooth = map2_dbl(.x = smooth_fits, .y = test, ~rmse(model = .x, data = .y))
  )
```


Let's look at the results...

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot()
```


```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin()
```

